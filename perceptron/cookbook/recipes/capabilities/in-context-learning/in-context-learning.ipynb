{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Capability â€” Single-Image In-Context Learning\n",
    "Guide the model with one exemplar image before detecting the same object in a different scene.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericpence/perceptron_repo/blob/main/cookbook/recipes/capabilities/in-context-learning/in-context-learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "Install the SDK plus helpers for drawing overlays inside the notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade perceptron pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Configure the Perceptron client\n",
    "Load the API key (inline or from the environment) once, then reuse the configured client for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from cookbook.utils import cookbook_asset\n",
    "from perceptron import annotate_image, bbox, configure, detect\n",
    "\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "configure(\n",
    "    provider=\"perceptron\",\n",
    "    # model=\"isaac-0.1\",  # Enable once the SDK supports the model argument.\n",
    ")\n",
    "\n",
    "EXAMPLE_IMAGE = cookbook_asset(\"in-context-learning\", \"single\", \"cake_mixer_example.webp\")\n",
    "TARGET_IMAGE = cookbook_asset(\"in-context-learning\", \"single\", \"find_kitchen_item.webp\")\n",
    "ANNOTATED_PATH = Path(\"find_kitchen_item_annotated.png\")\n",
    "\n",
    "for path in (EXAMPLE_IMAGE, TARGET_IMAGE):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing asset: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d01f7",
   "metadata": {},
   "source": [
    "## Bootstrap the exemplar box\n",
    "Run one detection pass on the exemplar image so we can feed a precise bounding box back as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765950a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = detect(\n",
    "    str(EXAMPLE_IMAGE),\n",
    "    classes=[\"objectCategory1\"],\n",
    "    max_outputs=1,\n",
    ")\n",
    "if not bootstrap.points:\n",
    "    raise RuntimeError(\"Detect returned no boxes for the exemplar. Adjust the label or image.\")\n",
    "\n",
    "first_box = bootstrap.points[0]\n",
    "example_shot = annotate_image(\n",
    "    str(EXAMPLE_IMAGE),\n",
    "    {\n",
    "        \"objectCategory1\": [\n",
    "            bbox(\n",
    "                int(first_box.top_left.x),\n",
    "                int(first_box.top_left.y),\n",
    "                int(first_box.bottom_right.x),\n",
    "                int(first_box.bottom_right.y),\n",
    "                mention=\"objectCategory1\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "collections = example_shot.get(\"collections\") or []\n",
    "boxes = example_shot.get(\"boxes\") or []\n",
    "placeholder = \"objectCategory1\"\n",
    "if collections:\n",
    "    exemplar_annotation = collections[0]\n",
    "elif boxes:\n",
    "    exemplar_annotation = boxes[0]\n",
    "else:\n",
    "    raise RuntimeError(\"annotate_image returned no collections or boxes; check the exemplar annotations.\")\n",
    "print(\"Prepared exemplar guidance with\", getattr(exemplar_annotation, \"mention\", placeholder) or placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## Detect the same object in a new scene\n",
    "Pass the exemplar annotation back to `detect` via the `examples` argument to nudge the model toward consistent grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect(\n",
    "    str(TARGET_IMAGE),\n",
    "    classes=[\"objectCategory1\"],\n",
    "    examples=[example_shot],\n",
    ")\n",
    "\n",
    "print(result.text)\n",
    "boxes = result.points or []\n",
    "print(f\"Returned {len(boxes)} grounded regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## Render the grounded output\n",
    "Overlay the predicted boxes on the target scene and preview the saved PNG inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(TARGET_IMAGE).convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "if boxes:\n",
    "\n",
    "    def to_px(point):\n",
    "        return point.x / 1000 * img.width, point.y / 1000 * img.height\n",
    "\n",
    "    for box in boxes:\n",
    "        top_left = to_px(box.top_left)\n",
    "        bottom_right = to_px(box.bottom_right)\n",
    "        draw.rectangle([top_left, bottom_right], outline=\"lime\", width=3)\n",
    "        label = box.mention or getattr(box, \"label\", None) or \"objectCategory1\"\n",
    "        text_position = (top_left[0], max(top_left[1] - 20, 0))\n",
    "        draw.text(text_position, label, fill=\"lime\", font=font)\n",
    "else:\n",
    "    print(\"No boxes returned; adjust the exemplar or label and rerun.\")\n",
    "\n",
    "img.save(ANNOTATED_PATH)\n",
    "display(IPyImage(filename=str(ANNOTATED_PATH)))\n",
    "print(f\"Saved annotated target to {ANNOTATED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## Conclusion & next steps\n",
    "- Swap in different exemplar / target pairs to explore other categories.\n",
    "- Add more than one exemplar by appending to the `examples` list for tougher distinctions.\n",
    "- Combine this flow with detection or Q&A helpers to build end-to-end pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
