{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Multi-image in-context learning\n",
    "Seed Isaac with cat vs. dog exemplars and detect the correct class in a new frame.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericpence/perceptron_repo/blob/main/cookbook/recipes/capabilities/multi-image-in-context-learning/multi-image-in-context-learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade perceptron pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Configure the SDK and resolve assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from cookbook.utils import cookbook_asset\n",
    "from perceptron import annotate_image, bbox, configure, detect\n",
    "\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "configure(\n",
    "    provider=\"perceptron\",\n",
    "    # model=\"isaac-0.1\",  # Enable once the SDK supports the model argument.\n",
    ")\n",
    "\n",
    "CAT_IMAGE = cookbook_asset(\"in-context-learning\", \"multi\", \"classA.jpg\")\n",
    "DOG_IMAGE = cookbook_asset(\"in-context-learning\", \"multi\", \"classB.webp\")\n",
    "TARGET_IMAGE = cookbook_asset(\"in-context-learning\", \"multi\", \"cat_dog_input.png\")\n",
    "for path in (CAT_IMAGE, DOG_IMAGE, TARGET_IMAGE):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing asset: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## Build the exemplar shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_example = annotate_image(\n",
    "    str(CAT_IMAGE),\n",
    "    {\n",
    "        \"classA\": [\n",
    "            bbox(316, 136, 703, 906, mention=\"classA\"),\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "dog_example = annotate_image(\n",
    "    str(DOG_IMAGE),\n",
    "    {\n",
    "        \"classB\": [\n",
    "            bbox(161, 48, 666, 980, mention=\"classB\"),\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## Detect the target frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect(\n",
    "    str(TARGET_IMAGE),\n",
    "    classes=[\"classA\", \"classB\"],\n",
    "    examples=[cat_example, dog_example],\n",
    ")\n",
    "\n",
    "print(result.text)\n",
    "boxes = result.points or []\n",
    "for box in boxes:\n",
    "    print(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## Preview the annotated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(TARGET_IMAGE).convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "def to_px(point):\n",
    "    return point.x / 1000 * img.width, point.y / 1000 * img.height\n",
    "\n",
    "\n",
    "for box in boxes:\n",
    "    top_left = to_px(box.top_left)\n",
    "    bottom_right = to_px(box.bottom_right)\n",
    "    draw.rectangle([top_left, bottom_right], outline=\"lime\", width=3)\n",
    "    label = box.mention or getattr(box, \"label\", None) or box.mention\n",
    "    text_position = (top_left[0], max(top_left[1] - 20, 0))\n",
    "    draw.text(text_position, label, fill=\"lime\", font=font)\n",
    "\n",
    "annotated_path = TARGET_IMAGE.with_name(f\"{TARGET_IMAGE.stem}_annotated.png\")\n",
    "img.save(annotated_path)\n",
    "print(f\"Saved annotated target to {annotated_path}\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": "## Conclusion & next steps\n- Add more exemplar shots (or additional classes) to make tougher distinctions.\n- Vary the prompt or classes to localize other objects across multiple example images.\n- Pair this workflow with the single-image ICL or detection notebooks to compare approaches."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
