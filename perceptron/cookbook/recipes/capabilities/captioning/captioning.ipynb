{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": "# Capability â€” Image Captioning\nGenerate concise or rich descriptions for any scene, optionally returning grounded regions alongside the text.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericpence/perceptron_repo/blob/main/cookbook/recipes/capabilities/captioning/captioning.ipynb)"
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": "## Install dependencies\nInstall the SDK and Pillow so we can preview local assets inline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade perceptron pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": "## Configure the Perceptron client\nSet your API key once, reuse the configured client for the rest of the notebook, and resolve the captioning assets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from cookbook.utils import cookbook_asset\n",
    "from perceptron import caption, configure\n",
    "\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "configure(\n",
    "    provider=\"perceptron\",\n",
    "    # model=\"isaac-0.1\",  # Enable once the SDK supports the model argument.\n",
    ")\n",
    "\n",
    "SUBURBAN_STREET = cookbook_asset(\"capabilities\", \"caption\", \"suburban_street.webp\")\n",
    "SOLAR_ARRAY = cookbook_asset(\"capabilities\", \"caption\", \"solar_array.webp\")\n",
    "SOLAR_ANNOTATED = Path(\"solar_array_annotated.png\")\n",
    "\n",
    "for path in (SUBURBAN_STREET, SOLAR_ARRAY):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing asset: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": "## Describe a suburban street\nThe simplest call requests a concise caption and returns plain text."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPyImage(filename=str(SUBURBAN_STREET)))\n",
    "street_caption = caption(str(SUBURBAN_STREET), style=\"concise\", expects=\"text\")\n",
    "print(street_caption.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": "## Request grounded details\nSwitch styles and ask for bounding boxes so downstream tooling can highlight each part of the caption."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_caption = caption(\n",
    "    str(SOLAR_ARRAY),\n",
    "    style=\"detailed\",\n",
    "    expects=\"box\",\n",
    ")\n",
    "print(solar_caption.text)\n",
    "boxes = solar_caption.points or []\n",
    "print(f\"Returned {len(boxes)} grounded snippets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(SOLAR_ARRAY).convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "if boxes:\n",
    "\n",
    "    def to_px(point):\n",
    "        return point.x / 1000 * img.width, point.y / 1000 * img.height\n",
    "\n",
    "    for idx, box in enumerate(boxes):\n",
    "        top_left = to_px(box.top_left)\n",
    "        bottom_right = to_px(box.bottom_right)\n",
    "        draw.rectangle([top_left, bottom_right], outline=\"orange\", width=3)\n",
    "        label = box.mention or f\"snippet {idx + 1}\"\n",
    "        draw.text((top_left[0], max(top_left[1] - 18, 0)), label, fill=\"orange\", font=font)\n",
    "else:\n",
    "    print(\"No grounded regions returned; set expects='box' to request them.\")\n",
    "\n",
    "img.save(SOLAR_ANNOTATED)\n",
    "display(IPyImage(filename=str(SOLAR_ANNOTATED)))\n",
    "print(f\"Saved annotated caption overlay to {SOLAR_ANNOTATED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": "## Conclusion & next steps\n- Adjust the `style` argument (`concise`, `detailed`) to match your UX.\n- Keep `expects=\"text\"` for pure narrative outputs or request grounding via `expects=\"box\"` / `\"point\"`.\n- Wrap this logic in a helper that iterates through folders to batch caption datasets."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
