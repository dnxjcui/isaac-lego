{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": "# Tutorial â€” Isaac 0.1 Frame-by-Frame\nApply the Isaac 0.1 model to consecutive frames from `surf.mp4`, then render bounding boxes back onto each frame for review.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericpence/perceptron_repo/blob/main/cookbook/recipes/tutorials/isaac_0.1_frame_by_frame/isaac_0.1_frame_by_frame.ipynb)"
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": "## Install dependencies\nInstall the SDK, OpenCV for video decoding, and Pillow for previewing annotated frames."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade perceptron opencv-python pillow tqdm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": "## Configure the Perceptron client\nLoad your API key from the environment (or inline) and configure the SDK once for the entire notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cookbook.utils import cookbook_asset\n",
    "from perceptron import configure, image, perceive, text\n",
    "\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "configure(\n",
    "    provider=\"perceptron\",\n",
    "    # model=\"isaac-0.1\",  # Enable once the SDK supports the model argument.\n",
    ")\n",
    "\n",
    "VIDEO_PATH = cookbook_asset(\"tutorials\", \"isaac_0.1_frame_by_frame\", \"surf.mp4\")\n",
    "FRAMES_DIR = Path(\"frames\")\n",
    "ANNOTATIONS_DIR = Path(\"frames_annotated\")\n",
    "OUTPUT_VIDEO = Path(\"surf_annotated.mp4\")\n",
    "\n",
    "if not VIDEO_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing asset: {VIDEO_PATH}\")\n",
    "\n",
    "FRAMES_DIR.mkdir(exist_ok=True)\n",
    "ANNOTATIONS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": "## Extract frames from the MP4\nDecode the source video into individual frames so we can run the SDK on each image."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path: Path, dest_dir: Path, stride: int = 1) -> list[Path]:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
    "\n",
    "    frame_paths: list[Path] = []\n",
    "    idx = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        if idx % stride == 0:\n",
    "            frame_path = dest_dir / f\"frame_{idx:05d}.jpg\"\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "            frame_paths.append(frame_path)\n",
    "            saved += 1\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved} frames (stride={stride})\")\n",
    "    return frame_paths\n",
    "\n",
    "\n",
    "FRAME_PATHS = extract_frames(VIDEO_PATH, FRAMES_DIR, stride=3)\n",
    "if not FRAME_PATHS:\n",
    "    raise RuntimeError(\"No frames extracted; check the video and stride settings.\")\n",
    "\n",
    "# Preview the first frame\n",
    "first_frame = FRAME_PATHS[0]\n",
    "display(IPyImage(filename=str(first_frame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": "## Detect surfers on each frame\nCall `detect` in a loop to localize surfers frame-by-frame, then overlay the normalized boxes onto each frame."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = [\"surfer\", \"surfboard\"]\n",
    "PROMPT = \"Find every surfer and surfboard in the frame. Return one bounding box per item.\"\n",
    "\n",
    "\n",
    "@perceive(expects=\"box\", allow_multiple=True)\n",
    "def detect_surfers(frame_path):\n",
    "    frame = image(frame_path)\n",
    "    return frame + text(PROMPT)\n",
    "\n",
    "\n",
    "all_detections = []\n",
    "\n",
    "for frame_path in tqdm(FRAME_PATHS, desc=\"Detecting frames\"):\n",
    "    result = detect_surfers(str(frame_path))\n",
    "    boxes = result.points or []\n",
    "    all_detections.append(\n",
    "        {\n",
    "            \"frame\": frame_path.name,\n",
    "            \"boxes_count\": len(boxes),\n",
    "            \"text\": result.text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    img = Image.open(frame_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    def to_px(point):\n",
    "        return point.x / 1000 * img.width, point.y / 1000 * img.height\n",
    "\n",
    "    for box in boxes:\n",
    "        top_left = to_px(box.top_left)\n",
    "        bottom_right = to_px(box.bottom_right)\n",
    "        draw.rectangle([top_left, bottom_right], outline=\"lime\", width=3)\n",
    "        label = box.mention or getattr(box, \"label\", None) or \"surfer\"\n",
    "        text_x = top_left[0]\n",
    "        text_y = max(top_left[1] - 18, 0)\n",
    "        draw.text((text_x, text_y), label, fill=\"lime\")\n",
    "\n",
    "    output_path = ANNOTATIONS_DIR / frame_path.name\n",
    "    img.save(output_path)\n",
    "\n",
    "print(f\"Annotated {len(FRAME_PATHS)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": "## Rebuild the annotated video\nStitch the saved frames back into an MP4 for quick review."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_video(frame_dir: Path, output_path: Path, fps: int = 30) -> None:\n",
    "    frames = sorted(frame_dir.glob(\"frame_*.jpg\"))\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No annotated frames found to stitch.\")\n",
    "\n",
    "    sample = cv2.imread(str(frames[0]))\n",
    "    height, width = sample.shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_path in frames:\n",
    "        frame = cv2.imread(str(frame_path))\n",
    "        writer.write(frame)\n",
    "    writer.release()\n",
    "    print(f\"Saved annotated video to {output_path}\")\n",
    "\n",
    "\n",
    "stitch_video(ANNOTATIONS_DIR, OUTPUT_VIDEO, fps=10)\n",
    "display(IPyImage(filename=str(ANNOTATIONS_DIR / FRAME_PATHS[0].name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": "## Conclusion & next steps\n- Adjust `TARGET_CLASSES` and `PROMPT` to match your use case.\n- Tune the `stride` and FPS values for longer videos.\n- Persist `all_detections` to JSON or a database for further analysis."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
