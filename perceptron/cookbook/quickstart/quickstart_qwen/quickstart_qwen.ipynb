{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": "# Quickstart â€” Running Qwen 3 VL\nLocalize every flower in the frame using the Qwen 3 VL model.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericpence/perceptron_repo/blob/main/cookbook/quickstart/qwen/quickstart_qwen.ipynb)\n"
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "Install the SDK plus helpers for environment loading and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade perceptron pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Configure the Perceptron client\n",
    "Load your API key from the environment (or `.env`) and configure the SDK once for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from cookbook.utils import cookbook_asset\n",
    "from perceptron import configure, image, perceive, text\n",
    "\n",
    "MODEL_NAME = \"qwen3-vl-235b-a22b-thinking\"\n",
    "\n",
    "\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "# configure() reads PERCEPTRON_API_KEY from the environment.\n",
    "configure(\n",
    "    provider=\"perceptron\",\n",
    "    # model=MODEL_NAME,  # Enable once the SDK supports the model argument.\n",
    ")\n",
    "\n",
    "IMAGE_PATH = cookbook_asset(\"quickstart\", \"qwen\", \"flowers.jpg\")\n",
    "ANNOTATED_PATH = Path(\"flowers_annotated.jpg\")\n",
    "\n",
    "if not IMAGE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected sample image at {IMAGE_PATH}. Add it via cookbook/_shared/assets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a10f8",
   "metadata": {},
   "source": "## Detect every flower\nCreate a reusable `@perceive` helper to prompt Qwen 3 VL, then draw the grounded boxes.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb309b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = [\"flower\"]\n",
    "\n",
    "\n",
    "@perceive(expects=\"box\", allow_multiple=True)\n",
    "def detect_flowers(frame_path):\n",
    "    scene = image(frame_path)\n",
    "    friendly_classes = \", \".join(TARGET_CLASSES)\n",
    "    return scene + text(\n",
    "        f\"Localize every {friendly_classes}. Return one bounding box per bloom and include a mention attribute.\"\n",
    "    )\n",
    "\n",
    "\n",
    "result = detect_flowers(str(IMAGE_PATH))\n",
    "\n",
    "img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "\n",
    "def to_px(point):\n",
    "    return point.x / 1000 * img.width, point.y / 1000 * img.height\n",
    "\n",
    "\n",
    "for idx, box in enumerate(result.points or []):\n",
    "    top_left = to_px(box.top_left)\n",
    "    bottom_right = to_px(box.bottom_right)\n",
    "    draw.rectangle([top_left, bottom_right], outline=\"lime\", width=3)\n",
    "    label = box.mention or getattr(box, \"label\", None) or f\"flower {idx + 1}\"\n",
    "    draw.text((top_left[0], max(top_left[1] - 12, 0)), label, fill=\"lime\")\n",
    "\n",
    "img.save(ANNOTATED_PATH)\n",
    "print(\"Annotated image saved to\", ANNOTATED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## Preview the annotated output\n",
    "Display the saved overlay directly in the notebook so you can confirm grounding without leaving the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANNOTATED_PATH.exists():\n",
    "    display(IPyImage(filename=str(ANNOTATED_PATH)))\n",
    "else:\n",
    "    print(\"Run the detection cell first to generate the annotated image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": "## Conclusion & next steps\n- Swap `TARGET_CLASSES` to label the objects you care about.\n- Chain more cookbook capabilities (captioning, OCR, visual QA, in-context learning, tutorials) to expand the workflow.\n- Convert the drawing logic into a reusable helper when you integrate with production tooling.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
